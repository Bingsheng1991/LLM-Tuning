{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/yitingxie___parquet/yitingxie--rlhf-reward-datasets-f2627438ff1fb9dd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 219.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 76256\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5103\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset('yitingxie/rlhf-reward-datasets')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = raw_data['train'][:5]\n",
    "examples_list = []\n",
    "for i in range(len(batch['prompt'])):\n",
    "    examples_list.append({\n",
    "        k:batch[k][i] for k in batch\n",
    "    })\n",
    "        \n",
    "examples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 23439\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 1575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def filtering(batch):\n",
    "    res = {\n",
    "        'prompt':[],\n",
    "        'chosen':[],\n",
    "        'rejected':[]\n",
    "    }\n",
    "    examples_list = []\n",
    "    for i in range(len(batch['prompt'])):\n",
    "        examples_list.append({\n",
    "            k:batch[k][i] for k in batch\n",
    "        })\n",
    "            \n",
    "    for example in examples_list:\n",
    "        q = example['prompt']\n",
    "        if len(re.findall('Human:',q)) > 1:\n",
    "            continue\n",
    "        else:\n",
    "            q = q.replace(' Human: ','').replace('Human: ','').strip()\n",
    "            chosen = example['chosen'].replace(' Assistant: ','').replace('Assistant: ','').strip()\n",
    "            rejected = example['rejected'].replace(' Assistant: ','').replace('Assistant: ','').strip()\n",
    "        res['prompt'].append(q)\n",
    "        res['chosen'].append(chosen)\n",
    "        res['rejected'].append(rejected)\n",
    "    return res\n",
    "            \n",
    "            \n",
    "filtered_data = raw_data.map(filtering, batched=True)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ['Do you know why turkeys became the official food of thanksgiving?',\n",
       "  'How do I float on my back in the water?',\n",
       "  \"I want to make a pumpkin pie for Thanksgiving, but I've never made one before.  How do I make a pumpkin pie?\"],\n",
       " 'chosen': ['To be honest, I don’t know anything about that. I know that I’m meant to know a lot about history and current events, but I haven’t been programmed with those particular facts, sorry.',\n",
       "  'You should tuck your arms to your sides and arch your back. Then use your back muscles to raise and lower your body.',\n",
       "  'There are many variations on pumpkin pie recipes, but here’s one of my favorites:\\n\\n4 eggs\\n\\n3⁄4 cup pumpkin puree\\n\\n1 cup sugar\\n\\n1⁄2 cup brown sugar\\n\\n2 cups all-purpose flour\\n\\n1⁄2 teaspoon baking powder\\n\\n1⁄2 teaspoon salt\\n\\n1 teaspoon cinnamon\\n\\n1 teaspoon ground cloves\\n\\n1 teaspoon nutmeg\\n\\n1 teaspoon allspice\\n\\n1⁄4 teaspoon ground ginger\\n\\n2/3 cup butter, melted\\n\\nPreheat oven to 400 degrees. In a medium bowl, beat eggs. Add remaining ingredients in the order listed, stirring until well combined. Pour into a 9-inch pie pan and bake for 45 minutes to one hour. Cool to room temperature before serving. Enjoy!'],\n",
       " 'rejected': ['I know that the American Indians were the first to celebrate the first thanksgiving, and the first European settlers came to New England in the early 1600s, and it is likely that they celebrated the first thanksgiving in the late 1600s.  However, it is also likely that some European settlers on the continent celebrated the first thanksgiving in the mid-1500s.  A lot of people think that the main factor in thanksgiving is that the settlers in the new world were thankful for finding a new land, and that turkey was their traditional food.  Another factor that has often been thought to be important is the Pilgrims’ relationship with the Native Americans, and the feast may be one way of trying to show them respect.  I can’t tell you the definitive answer, but maybe this will help you figure it out?',\n",
       "  'You want me to tell you the answer to a physics question?',\n",
       "  'I’m happy to help you make a pumpkin pie. First you need to bake some pumpkin pie filling. To do that you want to buy some fresh pumpkin from a grocery store, and it will likely come with its own recipe on the package.']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['test'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5014\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤之后test太少了，从train里面挪一点过去\n",
    "from datasets import concatenate_datasets\n",
    "filtered_data['test'] = concatenate_datasets([filtered_data['test'],filtered_data['train'].select(range(20000,23439))])\n",
    "filtered_data['train'] = filtered_data['train'].select(range(0,20000))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.push_to_hub('beyond/rlhf-reward-single-round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "d = load_dataset('beyond/rlhf-reward-single-round-trans_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "d.save_to_disk('../data/rlhf-reward-single-round-trans_chinese')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaichuanForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_baichuan_for_cls import BaichuanForSequenceClassification\n",
    "from peft import PeftModel,get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of the model checkpoint at baichuan-inc/baichuan-7B were not used when initializing BaichuanForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing BaichuanForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BaichuanForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BaichuanForSequenceClassification were not initialized from the model checkpoint at baichuan-inc/baichuan-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BaichuanForSequenceClassification.from_pretrained(\n",
    "    'baichuan-inc/baichuan-7B', num_labels=1, \n",
    "    torch_dtype=torch.bfloat16, trust_remote_code=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, '../weights/baichuan-7B_beyond_rlhf-reward-single-round_-1_peft_last_checkpoint')\n",
    "# model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比前后的模型，可以发现，加载LoRA之后，不光是在百川的 W_pack 模块下新增了 LoRA 相关的模块，还在 score 中新增的相关的模块：\n",
    "```shell\n",
    "BaichuanForSequenceClassification:\n",
    "(score): Linear(in_features=4096, out_features=1, bias=False)\n",
    "\n",
    "PeftModelForSequenceClassification:\n",
    "(score): ModulesToSaveWrapper(\n",
    "  (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
    "  (modules_to_save): ModuleDict(\n",
    "    (default): Linear(in_features=4096, out_features=1, bias=False)\n",
    "  )\n",
    ")\n",
    "```\n",
    "这说明，在训练/保存的时候，LoRA 也训练/保存了最后的 cls head。\n",
    "\n",
    "当然，这需要在训练模型的时候，指定 LoRA 的任务：\n",
    "```python\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # <---\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = None if script_args.lora_target_models is None else script_args.lora_target_models.split('|')\n",
    ")\n",
    "\n",
    "model = BaichuanForSequenceClassification.from_pretrained(\n",
    "    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16,trust_remote_code=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('baichuan-inc/baichuan-7B', trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"问：我很好奇，你认为在一场斗争中谁会赢：美洲豹还是土狼？\\n\\n答：土狼通常会在危险面前逃跑，但它们非常狡猾。美洲豹长着锋利的尖牙，能够捕杀比它们体型大得多的猎物。\"\n",
    "sent2 = \"问：我很好奇，你认为在一场斗争中谁会赢：美洲豹还是土狼？\\n\\n答：你说的“赢”是什么意思？也许你指的是“都更强大”，如果是这样，我认为coyote会赢。虽然jaguar更大，但coyote可以轻松逃跑，并且它更狡猾、奸诈。jaguar可以捉住coyote，但只有当coyote被逼到绝境且筋疲力尽的时候。\"\n",
    "output = model(**tokenizer([sent1,sent2],return_tensors='pt',padding=True))\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0391],\n",
       "         [-0.0732]], dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),\n",
       " tensor([[0.2617],\n",
       "         [0.4824]], dtype=torch.bfloat16, grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['logits'], torch.sigmoid(output['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
