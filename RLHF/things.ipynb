{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/yitingxie___parquet/yitingxie--rlhf-reward-datasets-f2627438ff1fb9dd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 219.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 76256\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5103\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset('yitingxie/rlhf-reward-datasets')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = raw_data['train'][:5]\n",
    "examples_list = []\n",
    "for i in range(len(batch['prompt'])):\n",
    "    examples_list.append({\n",
    "        k:batch[k][i] for k in batch\n",
    "    })\n",
    "        \n",
    "examples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 23439\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 1575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def filtering(batch):\n",
    "    res = {\n",
    "        'prompt':[],\n",
    "        'chosen':[],\n",
    "        'rejected':[]\n",
    "    }\n",
    "    examples_list = []\n",
    "    for i in range(len(batch['prompt'])):\n",
    "        examples_list.append({\n",
    "            k:batch[k][i] for k in batch\n",
    "        })\n",
    "            \n",
    "    for example in examples_list:\n",
    "        q = example['prompt']\n",
    "        if len(re.findall('Human:',q)) > 1:\n",
    "            continue\n",
    "        else:\n",
    "            q = q.replace(' Human: ','').replace('Human: ','').strip()\n",
    "            chosen = example['chosen'].replace(' Assistant: ','').replace('Assistant: ','').strip()\n",
    "            rejected = example['rejected'].replace(' Assistant: ','').replace('Assistant: ','').strip()\n",
    "        res['prompt'].append(q)\n",
    "        res['chosen'].append(chosen)\n",
    "        res['rejected'].append(rejected)\n",
    "    return res\n",
    "            \n",
    "            \n",
    "filtered_data = raw_data.map(filtering, batched=True)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ['Do you know why turkeys became the official food of thanksgiving?',\n",
       "  'How do I float on my back in the water?',\n",
       "  \"I want to make a pumpkin pie for Thanksgiving, but I've never made one before.  How do I make a pumpkin pie?\"],\n",
       " 'chosen': ['To be honest, I don’t know anything about that. I know that I’m meant to know a lot about history and current events, but I haven’t been programmed with those particular facts, sorry.',\n",
       "  'You should tuck your arms to your sides and arch your back. Then use your back muscles to raise and lower your body.',\n",
       "  'There are many variations on pumpkin pie recipes, but here’s one of my favorites:\\n\\n4 eggs\\n\\n3⁄4 cup pumpkin puree\\n\\n1 cup sugar\\n\\n1⁄2 cup brown sugar\\n\\n2 cups all-purpose flour\\n\\n1⁄2 teaspoon baking powder\\n\\n1⁄2 teaspoon salt\\n\\n1 teaspoon cinnamon\\n\\n1 teaspoon ground cloves\\n\\n1 teaspoon nutmeg\\n\\n1 teaspoon allspice\\n\\n1⁄4 teaspoon ground ginger\\n\\n2/3 cup butter, melted\\n\\nPreheat oven to 400 degrees. In a medium bowl, beat eggs. Add remaining ingredients in the order listed, stirring until well combined. Pour into a 9-inch pie pan and bake for 45 minutes to one hour. Cool to room temperature before serving. Enjoy!'],\n",
       " 'rejected': ['I know that the American Indians were the first to celebrate the first thanksgiving, and the first European settlers came to New England in the early 1600s, and it is likely that they celebrated the first thanksgiving in the late 1600s.  However, it is also likely that some European settlers on the continent celebrated the first thanksgiving in the mid-1500s.  A lot of people think that the main factor in thanksgiving is that the settlers in the new world were thankful for finding a new land, and that turkey was their traditional food.  Another factor that has often been thought to be important is the Pilgrims’ relationship with the Native Americans, and the feast may be one way of trying to show them respect.  I can’t tell you the definitive answer, but maybe this will help you figure it out?',\n",
       "  'You want me to tell you the answer to a physics question?',\n",
       "  'I’m happy to help you make a pumpkin pie. First you need to bake some pumpkin pie filling. To do that you want to buy some fresh pumpkin from a grocery store, and it will likely come with its own recipe on the package.']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['test'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5014\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤之后test太少了，从train里面挪一点过去\n",
    "from datasets import concatenate_datasets\n",
    "filtered_data['test'] = concatenate_datasets([filtered_data['test'],filtered_data['train'].select(range(20000,23439))])\n",
    "filtered_data['train'] = filtered_data['train'].select(range(0,20000))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.push_to_hub('beyond/rlhf-reward-single-round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "d = load_dataset('beyond/rlhf-reward-single-round-trans_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "d.save_to_disk('../data/rlhf-reward-single-round-trans_chinese')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaichuanForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_baichuan_for_cls import BaichuanForSequenceClassification\n",
    "from peft import PeftModel,get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaichuanForSequenceClassification.from_pretrained(\n",
    "    'baichuan-inc/baichuan-7B', num_labels=1, \n",
    "    torch_dtype=torch.bfloat16, trust_remote_code=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, '../weights/baichuan-7B_beyond_rlhf-reward-single-round_-1_peft_last_checkpoint')\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比前后的模型，可以发现，加载LoRA之后，不光是在百川的 W_pack 模块下新增了 LoRA 相关的模块，还在 score 中新增的相关的模块：\n",
    "```shell\n",
    "BaichuanForSequenceClassification:\n",
    "(score): Linear(in_features=4096, out_features=1, bias=False)\n",
    "\n",
    "PeftModelForSequenceClassification:\n",
    "(score): ModulesToSaveWrapper(\n",
    "  (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
    "  (modules_to_save): ModuleDict(\n",
    "    (default): Linear(in_features=4096, out_features=1, bias=False)\n",
    "  )\n",
    ")\n",
    "```\n",
    "这说明，在训练/保存的时候，LoRA 也训练/保存了最后的 cls head。\n",
    "\n",
    "当然，这需要在训练模型的时候，指定 LoRA 的任务：\n",
    "```python\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # <---\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = None if script_args.lora_target_models is None else script_args.lora_target_models.split('|')\n",
    ")\n",
    "\n",
    "model = BaichuanForSequenceClassification.from_pretrained(\n",
    "    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16,trust_remote_code=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
